import streamlit as st
import pandas as pd
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from webdriver_manager.core.os_manager import ChromeType
from bs4 import BeautifulSoup
from collections import Counter
import time
from io import BytesIO
from docx import Document

# --- C·∫§U H√åNH TRANG ---
st.set_page_config(page_title="SEO Content Researcher", layout="wide")

# CSS t√πy ch·ªânh cho ƒë·∫πp
st.markdown("""
<style>
    .main {background-color: #f4f6f9;}
    h1 {color: #2c3e50; font-family: 'Helvetica', sans-serif;}
    .stButton>button {width: 100%; border-radius: 5px; font-weight: bold;}
    .stTextArea textarea {font-family: monospace;}
</style>
""", unsafe_allow_html=True)

# --- QU·∫¢N L√ù TR·∫†NG TH√ÅI (SESSION STATE) ---
if 'results' not in st.session_state:
    st.session_state['results'] = None
if 'is_analyzed' not in st.session_state:
    st.session_state['is_analyzed'] = False

# --- H√ÄM C√ÄI ƒê·∫∂T DRIVER ---
@st.cache_resource
def get_driver():
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    service = Service(ChromeDriverManager(chrome_type=ChromeType.CHROMIUM).install())
    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver

# --- H√ÄM L√ÄM S·∫†CH HTML (CLEANER) ---
def clean_html(soup):
    # 1. X√≥a c√°c th·∫ª k·ªπ thu·∫≠t & th·∫ª ƒëi·ªÅu h∆∞·ªõng ch·∫Øc ch·∫Øn kh√¥ng ch·ª©a content
    for tag in soup(['script', 'style', 'header', 'footer', 'nav', 'aside', 'noscript', 'form', 'iframe']):
        tag.decompose()
    
    # 2. X√≥a theo Class/ID r√°c (Sidebar, Menu, Related, Comments)
    # C√°c t·ª´ kh√≥a th∆∞·ªùng g·∫∑p trong class c·ªßa web
    garbage_keywords = [
        'sidebar', 'widget', 'menu', 'nav', 'comment', 'share', 'social', 
        'popup', 'modal', 'cookie', 'related', 'author-box', 'breadcrumb', 'footer'
    ]
    
    for tag in soup.find_all(True):
        # Ki·ªÉm tra class v√† id c·ªßa th·∫ª
        check_list = (tag.get('class') or []) + ([tag.get('id')] if tag.get('id') else [])
        check_str = " ".join(check_list).lower()
        
        if any(kw in check_str for kw in garbage_keywords):
            tag.decompose()
            
    return soup

# --- GIAO DI·ªÜN CH√çNH ---
st.title("üîé SEO Content Researcher & Outline Generator")

if not st.session_state['is_analyzed']:
    # M√†n h√¨nh nh·∫≠p li·ªáu
    with st.container():
        st.info("üí° Tool h·ªó tr·ª£ qu√©t n·ªôi dung JS, ch·∫∑n bot (An C∆∞·ªùng, v.v). T·ªëi ƒëa 5 URL.")
        urls_input = st.text_area("üëâ D√°n danh s√°ch URL (M·ªói d√≤ng 1 link):", height=200)
        
        if st.button("üöÄ B·∫ÆT ƒê·∫¶U PH√ÇN T√çCH", type="primary"):
            if not urls_input.strip():
                st.warning("Vui l√≤ng nh·∫≠p √≠t nh·∫•t 1 URL!")
            else:
                url_list = [x.strip() for x in urls_input.split('\n') if x.strip()][:5] # Gi·ªõi h·∫°n 5 URL
                
                # --- B·∫ÆT ƒê·∫¶U QU√âT ---
                progress_bar = st.progress(0)
                status_text = st.empty()
                
                all_data = [] # D·ªØ li·ªáu th√¥ ƒë·ªÉ xu·∫•t Excel
                outline_corpus = [] # Ch·ª©a H2 ƒë·ªÉ t√¨m ƒëi·ªÉm chung
                display_text_full = "" # Bi·∫øn ch·ª©a text hi·ªÉn th·ªã tr√™n web
                
                try:
                    driver = get_driver()
                    
                    for i, url in enumerate(url_list):
                        status_text.text(f"‚è≥ ƒêang x·ª≠ l√Ω ({i+1}/{len(url_list)}): {url}")
                        
                        page_data = {'URL': url, 'Title': '', 'Meta Desc': '', 'Headings': []}
                        
                        try:
                            driver.get(url)
                            time.sleep(3) # Ch·ªù JS load
                            soup = BeautifulSoup(driver.page_source, 'html.parser')
                            
                            # 1. L·∫•y SEO Title & Meta (Tr∆∞·ªõc khi clean)
                            if soup.title:
                                page_data['Title'] = soup.title.get_text(strip=True)
                            
                            meta = soup.find('meta', attrs={'name': 'description'}) or soup.find('meta', attrs={'property': 'og:description'})
                            if meta:
                                page_data['Meta Desc'] = meta.get('content', '').strip()

                            # 2. Clean HTML
                            soup = clean_html(soup)
                            
                            # 3. L·∫•y Heading
                            headings = soup.find_all(['h1', 'h2', 'h3'])
                            heading_list_text = []
                            
                            # Format text hi·ªÉn th·ªã
                            display_text_full += f"URL: {url}\n"
                            display_text_full += f"TITLE: {page_data['Title']}\n"
                            display_text_full += f"META: {page_data['Meta Desc']}\n"
                            display_text_full += "STRUCTURE:\n"

                            for tag in headings:
                                txt = tag.get_text(strip=True)
                                if txt:
                                    tag_name = tag.name.upper()
                                    heading_list_text.append(f"[{tag_name}] {txt}")
                                    display_text_full += f"- [{tag_name}] {txt}\n"
                                    
                                    if tag.name == 'h2':
                                        outline_corpus.append(txt)

                            display_text_full += "\n" + "="*50 + "\n\n"
                            page_data['Headings'] = "\n".join(heading_list_text) # L∆∞u d·∫°ng chu·ªói ƒë·ªÉ cho v√†o Excel
                            
                            all_data.append(page_data)

                        except Exception as e:
                            st.error(f"L·ªói khi ƒë·ªçc {url}: {e}")
                        
                        progress_bar.progress((i + 1) / len(url_list))
                    
                    # X·ª≠ l√Ω Logic Outline Recommend
                    recommend_outline = []
                    if outline_corpus:
                        # L√†m s·∫°ch text ƒë·ªÉ so s√°nh
                        normalized_h2 = [h.lower().replace('l√† g√¨','').replace('nh∆∞ th·∫ø n√†o','').strip() for h in outline_corpus]
                        # ƒê·∫øm t·∫ßn su·∫•t
                        counter = Counter(normalized_h2)
                        
                        # Logic: L·∫•y c√°c √Ω xu·∫•t hi·ªán > 1 l·∫ßn, n·∫øu √≠t qu√° th√¨ l·∫•y top 10
                        most_common = counter.most_common(15)
                        recommend_text = "G·ª¢I √ù OUTLINE (D·ª±a tr√™n t·∫ßn su·∫•t xu·∫•t hi·ªán):\n"
                        for topic, count in most_common:
                            original_text = next((h for h in outline_corpus if h.lower().replace('l√† g√¨','').replace('nh∆∞ th·∫ø n√†o','').strip() == topic), topic.title())
                            note = f"(x{count} web nh·∫Øc ƒë·∫øn)" if count > 1 else ""
                            recommend_text += f"- {original_text} {note}\n"
                            recommend_outline.append(original_text)
                    else:
                        recommend_text = "Kh√¥ng ƒë·ªß d·ªØ li·ªáu H2 ƒë·ªÉ ƒë·ªÅ xu·∫•t outline."

                    # L∆ØU V√ÄO SESSION STATE
                    st.session_state['results'] = {
                        'all_data': all_data,
                        'display_text': display_text_full,
                        'recommend_text': recommend_text,
                        'recommend_list': recommend_outline
                    }
                    st.session_state['is_analyzed'] = True
                    st.rerun() # Load l·∫°i trang ƒë·ªÉ chuy·ªÉn sang m√†n h√¨nh k·∫øt qu·∫£

                except Exception as e:
                    st.error(f"L·ªói kh·ªüi ƒë·ªông Driver: {e}")

else:
    # --- M√ÄN H√åNH K·∫æT QU·∫¢ ---
    res = st.session_state['results']
    
    # N√∫t Tr·ªü l·∫°i (Reset)
    if st.button("‚¨ÖÔ∏è TR·ªû L·∫†I (F5 ƒë·ªÉ research m·ªõi)"):
        st.session_state['results'] = None
        st.session_state['is_analyzed'] = False
        st.rerun()

    col1, col2 = st.columns(2)
    
    # 1. Hi·ªÉn th·ªã Outline Recommend
    with col1:
        st.subheader("üí° Outline ƒê·ªÅ Xu·∫•t")
        st.text_area("Copy Outline:", value=res['recommend_text'], height=400)
    
    # 2. Hi·ªÉn th·ªã Chi ti·∫øt Research
    with col2:
        st.subheader("üìù D·ªØ li·ªáu chi ti·∫øt (Raw)")
        st.text_area("To√†n b·ªô Title, Meta, Heading:", value=res['display_text'], height=400)

    st.divider()
    st.subheader("üìÇ Xu·∫•t D·ªØ Li·ªáu")
    
    c1, c2 = st.columns(2)
    
    # --- N√öT XU·∫§T WORD (DOCX) ---
    doc = Document()
    doc.add_heading('B√ÅO C√ÅO NGHI√äN C·ª®U SEO', 0)
    
    doc.add_heading('PH·∫¶N 1: OUTLINE ƒê·ªÄ XU·∫§T', level=1)
    for line in res['recommend_list']:
        doc.add_paragraph(f"- {line}", style='List Bullet')
        
    doc.add_heading('PH·∫¶N 2: CHI TI·∫æT ƒê·ªêI TH·ª¶', level=1)
    for item in res['all_data']:
        doc.add_heading(item['URL'], level=2)
        doc.add_paragraph(f"SEO Title: {item['Title']}")
        doc.add_paragraph(f"Meta Desc: {item['Meta Desc']}")
        doc.add_paragraph("Headings:")
        doc.add_paragraph(item['Headings'])
        doc.add_paragraph("-" * 20)

    buffer_doc = BytesIO()
    doc.save(buffer_doc)
    buffer_doc.seek(0)
    
    with c1:
        st.download_button(
            label="üìÑ T·∫£i file Word (.docx)",
            data=buffer_doc,
            file_name="SEO_Report.docx",
            mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            type="primary"
        )

    # --- N√öT XU·∫§T EXCEL (2 SHEETS) ---
    buffer_xls = BytesIO()
    with pd.ExcelWriter(buffer_xls, engine='xlsxwriter') as writer:
        # Sheet 1: Outline Research
        # Chu·∫©n b·ªã data cho Sheet 1
        sheet1_data = []
        for idx, item in enumerate(res['all_data']):
            sheet1_data.append({
                'No.': idx + 1,
                'URL': item['URL'],
                'Title SEO': item['Title'],
                'Headings (H2-H3)': item['Headings'], # ƒê√£ format xu·ªëng d√≤ng trong text
                'Meta description': item['Meta Desc']
            })
        df1 = pd.DataFrame(sheet1_data)
        df1.to_excel(writer, sheet_name='Outline Research', index=False)
        
        # Format c·ªôt Headings cho d·ªÖ ƒë·ªçc (Wrap text)
        workbook = writer.book
        worksheet1 = writer.sheets['Outline Research']
        format_wrap = workbook.add_format({'text_wrap': True, 'valign': 'top'})
        worksheet1.set_column('D:D', 50, format_wrap) # C·ªôt Headings r·ªông ra
        worksheet1.set_column('B:B', 30) # URL
        worksheet1.set_column('C:C', 30) # Title
        worksheet1.set_column('E:E', 40, format_wrap) # Meta

        # Sheet 2: Outline Recommend
        df2 = pd.DataFrame(res['recommend_list'], columns=['Recommended H2'])
        df2.to_excel(writer, sheet_name='Outline Recommend', index=False)
        
        # Format Sheet 2
        worksheet2 = writer.sheets['Outline Recommend']
        worksheet2.set_column('A:A', 60)

    buffer_xls.seek(0)
    
    with c2:
        st.download_button(
            label="üìä T·∫£i file Excel (2 Sheets)",
            data=buffer_xls,
            file_name="SEO_Research_Data.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            type="primary"
        )